# General LLM Stuff

[My 5 Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023)

- "Decomposing Language Models Into Understandable Components" by Anthropic, because it showed me that interpretability in LLMs might be a solvable engineering problem, scaling similarly to LLMs themselves. The possibility of interpreting how LLMs form concepts and arrive at answers is truly fascinating.
    - https://www.anthropic.com/index/decomposing-language-models-into-understandable-components
- "Textbooks are All You Need" by Microsoft Research, which helped me understand that small LLMs trained on high-quality data can outperform much larger models. This paper inspired the Phi model and its subsequent Phi 1.5 and Phi 2 releases.
    - https://arxiv.org/abs/2306.11644
- "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation" also by Microsoft Research, introduced new ideas about multi-agent communication patterns. This could be one of the most significant recent papers on autonomous agents.
    - https://www.microsoft.com/en-us/research/publication/autogen-enabling-next-gen-llm-applications-via-multi-agent-conversation-framework/
- "Gorilla: Large Language Model Connected with Massive APIs" from Berkeley University, which challenged traditional RAG concepts and demonstrated that fine-tuning models on API datasets can yield incredible results. Other papers in this area such as ToolLLM were quite influential but I wanted to pick one.
    - https://arxiv.org/abs/2305.15334
- "FunSearch: Making new discoveries in mathematical sciences using Large Language Models" by Google DeepMind, as I am fascinated by applying AI to science, and discovering new computer science algorithms is exceptionally complex.
    - https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/
